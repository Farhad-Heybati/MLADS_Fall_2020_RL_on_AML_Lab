{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Training â€“ Single Job Set\n",
    "\n",
    "In this notebook we train a RL agent to solve the resource allocation problem, as described in the exploration notebook.\n",
    "\n",
    "In this implementation, we train using a single job set. This means the agent sees the same single set of jobs at each simulation repeatedly during training.\n",
    "\n",
    "The point here is to show that the agent can learn a policy that is better than both the random and shortest-job-first policies as shown in the exploration notebook, but is not able to generalize to an unseen job set, different than the one used during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that you are running this notebook in a Compute Instance on Azure ML with the Azure ML SDK version 1.17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure Machine Learning SDK Version: 1.17.0\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "\n",
    "# Check core SDK version number\n",
    "print('Azure Machine Learning SDK Version:', azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a reference to your Azure ML Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an Azure ML Experiment to track the agent training runs in your Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.experiment import Experiment\n",
    "\n",
    "# Experiment name\n",
    "experiment_name = 'deeprm_single_jobset_rllib_pg'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an Azure ML Compute Target for the head node of the Ray cluster. The actual RLLib training process happens in the Ray head node. Here we define a Compute Target having only one node. We use a virtual machine type having a GPU to accelerate the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found head compute target. just use it head-gpu\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "# Choose a name for the Ray head cluster\n",
    "head_compute_name = 'head-gpu'\n",
    "head_compute_min_nodes = 0\n",
    "head_compute_max_nodes = 1\n",
    "\n",
    "# This example uses GPU VM. For using CPU VM, set SKU to STANDARD_D2_V2\n",
    "head_vm_size = 'STANDARD_NC6'\n",
    "\n",
    "# Virtual network name\n",
    "vnet_name = 'your_vnet'\n",
    "\n",
    "if head_compute_name in ws.compute_targets:\n",
    "    head_compute_target = ws.compute_targets[head_compute_name]\n",
    "    if head_compute_target and type(head_compute_target) is AmlCompute:\n",
    "        if head_compute_target.provisioning_state == 'Succeeded':\n",
    "            print('found head compute target. just use it', head_compute_name)\n",
    "        else: \n",
    "            raise Exception(\n",
    "                'found head compute target but it is in state', head_compute_target.provisioning_state)\n",
    "else:\n",
    "    print('creating a new head compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=head_vm_size,\n",
    "        min_nodes=head_compute_min_nodes, \n",
    "        max_nodes=head_compute_max_nodes,\n",
    "        vnet_resourcegroup_name=ws.resource_group,\n",
    "        vnet_name=vnet_name,\n",
    "        subnet_name='default')\n",
    "\n",
    "    # Create the cluster\n",
    "    head_compute_target = ComputeTarget.create(ws, head_compute_name, provisioning_config)\n",
    "    \n",
    "    # Can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # If no min node count is provided it will use the scale settings for the cluster\n",
    "    head_compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "    # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(head_compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an Azure ML Compute Target for the worker nodes of the Ray cluster. Ray worker nodes run the environment rollout processes, where the RL agent interacts with the RL environment, executing actions given by the policy model being trained and collecting state observations and rewards. Here we create a Compute Target having two nodes. We use virtual machine types having 4 CPUs core each.\n",
    "\n",
    "We will have in total 8 CPUs from the worker nodes plus 6 from the head node. You will notice later that we specify 12 worker processes for RLLib, each allocating up to 1 CPU, and each running 5 RL environment simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found worker compute target. just use it worker-cpu\n"
     ]
    }
   ],
   "source": [
    "# Choose a name for your Ray worker compute target\n",
    "worker_compute_name = 'worker-cpu'\n",
    "worker_compute_min_nodes = 0 \n",
    "worker_compute_max_nodes = 2\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "worker_vm_size = 'STANDARD_D4_V3'\n",
    "\n",
    "# Create the compute target if it hasn't been created already\n",
    "if worker_compute_name in ws.compute_targets:\n",
    "    worker_compute_target = ws.compute_targets[worker_compute_name]\n",
    "    if worker_compute_target and type(worker_compute_target) is AmlCompute:\n",
    "        if worker_compute_target.provisioning_state == 'Succeeded':\n",
    "            print('found worker compute target. just use it', worker_compute_name)\n",
    "        else: \n",
    "            raise Exception(\n",
    "                'found worker compute target but it is in state', head_compute_target.provisioning_state)\n",
    "else:\n",
    "    print('creating a new worker compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=worker_vm_size,\n",
    "        min_nodes=worker_compute_min_nodes,\n",
    "        max_nodes=worker_compute_max_nodes,\n",
    "        vnet_resourcegroup_name=ws.resource_group,\n",
    "        vnet_name=vnet_name,\n",
    "        subnet_name='default')\n",
    "\n",
    "    # Create the compute target\n",
    "    worker_compute_target = ComputeTarget.create(ws, worker_compute_name, provisioning_config)\n",
    "    \n",
    "    # Can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # If no min node count is provided it will use the scale settings for the cluster\n",
    "    worker_compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "    # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(worker_compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a *WorkerConfiguration* using your worker compute target. We specify the number of nodes in the worker compute target to be used for training and additional PIP packages to install on those nodes as a part of setup. In this case, we define the PIP packages as dependencies for both head and worker nodes. With this setup, the game simulations will run directly on the worker compute nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.train.rl import WorkerConfiguration\n",
    "\n",
    "# Pip packages we will use for both head and worker\n",
    "pip_packages=['ray[rllib]==0.8.5', 'torch'] # version of Ray tested with Azure ML\n",
    "\n",
    "# Specify the Ray worker configuration\n",
    "worker_conf = WorkerConfiguration(\n",
    "    \n",
    "    # Azure Machine Learning compute target to run Ray workers\n",
    "    compute_target=worker_compute_target, \n",
    "    \n",
    "    # Number of worker nodes\n",
    "    node_count=2,\n",
    "    \n",
    "    # GPU\n",
    "    use_gpu=False, \n",
    "    \n",
    "    # PIP packages to use\n",
    "    pip_packages=pip_packages\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate reinforcement learning, Azure Machine Learning Python SDK provides a high level abstraction, the *ReinforcementLearningEstimator* class, which allows users to easily construct reinforcement learning run configurations for the underlying reinforcement learning framework. Reinforcement Learning in Azure Machine Learning supports the open source [Ray framework](https://ray.io/) and its highly customizable [RLLib](https://ray.readthedocs.io/en/latest/rllib.html#rllib-scalable-reinforcement-learning).\n",
    "\n",
    "The *ReinforcementLearningEstimator* is used to submit a job to Azure Machine Learning to start the Ray experiment run. We define the training script parameters here that will be passed to the estimator.\n",
    "\n",
    "Those parameters include:\n",
    "- Parameters for the RL algorithm implemented in RLLib, and the underlying Ray infrastructure. We are using Policy Gradients in our implementation.\n",
    "- Parameters for the RL environment. Here we specify only the simulation length and number of job sets.\n",
    "\n",
    "The *ReinforcementLearningEstimator* specifies the location of the training scripts and associated files by the *source_directory* parameter. There we have the training script, specified by the *entry_script* parameter and our custom environment file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.train.rl import ReinforcementLearningEstimator, Ray\n",
    "\n",
    "# Training script parameters\n",
    "script_params = {\n",
    "    # parameter for Ray\n",
    "    '--default_ray_address': 'localhost:6379',\n",
    "    # parameters for RLLib\n",
    "    '--gamma': 0.99,\n",
    "    '--num_gpus': 1,\n",
    "    '--num_workers': 12,\n",
    "    '--num_envs_per_worker': 5,\n",
    "    '--num_cpus_per_worker': 1,\n",
    "    '--use_pytorch': 1,\n",
    "    '--timesteps_per_iteration': 100000,\n",
    "    '--rollout_fragment_length': 50,\n",
    "    '--train_batch_size': 500,\n",
    "    '--lr': 0.00025,\n",
    "    '--num_iterations': 300,\n",
    "    # parameters for the resource allocation environment\n",
    "    '--simu_len': 50,\n",
    "    '--num_ex': 1\n",
    "}\n",
    "\n",
    "#  Reinforcement learning estimator\n",
    "rl_estimator = ReinforcementLearningEstimator(\n",
    "    \n",
    "    # Location of source files\n",
    "    source_directory='training_scripts',\n",
    "    \n",
    "    # Python script file\n",
    "    entry_script='deeprm_rllib_pg_train.py',\n",
    "    \n",
    "    # Parameters to pass to the script file\n",
    "    # Defined above.\n",
    "    script_params=script_params,\n",
    "    \n",
    "    # The Azure Machine Learning compute target set up for Ray head nodes\n",
    "    compute_target=head_compute_target,\n",
    "    \n",
    "    # Pip packages\n",
    "    pip_packages=pip_packages,\n",
    "    \n",
    "    # GPU usage\n",
    "    use_gpu=True,\n",
    "    \n",
    "    # Reinforcement learning framework. Currently must be Ray.\n",
    "    rl_framework=Ray(),\n",
    "    \n",
    "    # Ray worker configuration defined above.\n",
    "    worker_configuration=worker_conf,\n",
    "    \n",
    "    # How long to wait for whole cluster to start\n",
    "    cluster_coordination_timeout_seconds=3600,\n",
    "    \n",
    "    # Maximum time for the whole Ray job to run\n",
    "    # This will cut off the run after an hour\n",
    "    #max_run_duration_seconds=3600,\n",
    "    \n",
    "    # Allow the docker container Ray runs in to make full use\n",
    "    # of the shared memory available from the host OS.\n",
    "    shm_size='16g'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the estimator to start a run in Azure ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(config=rl_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitor and view results using the Azure Machine Learning Jupyter widget to see the status of your runs in real time. In our implementation, the widget shows two child runs: one for head and one for workers.\n",
    "\n",
    "When the *Status* in the table below shows *Running*, you can click on the onr corresponding to the head node (the lower run number) and a new window will open showing the metrics being captured by the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828c711972e243c4b7a45a4400342ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_RLWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'sdk_vâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/deeprm_single_jobset_rllib_pg/runs/deeprm_single_jobset_rllib_pg_1604867900_74b66959?wsid=/subscriptions/a6c2a7cc-d67e-4a1a-b765-983f08c0423a/resourcegroups/alvilcek-ml-rg2/workspaces/alvilcek-ml-workspace\", \"run_id\": \"deeprm_single_jobset_rllib_pg_1604867900_74b66959\", \"run_properties\": {\"run_id\": \"deeprm_single_jobset_rllib_pg_1604867900_74b66959\", \"created_utc\": \"2020-11-08T20:38:25.344703Z\", \"properties\": {}, \"tags\": {\"cluster_coordination_timeout_seconds\": \"3600\"}, \"end_time_utc\": \"2020-11-08T21:30:29.459505Z\", \"status\": \"Completed\", \"log_files\": {\"azureml-logs/reinforcementlearning.txt\": \"https://alvilcekmlwork8166615291.blob.core.windows.net/azureml/ExperimentRun/dcid.deeprm_single_jobset_rllib_pg_1604867900_74b66959/azureml-logs/reinforcementlearning.txt?sv=2019-02-02&sr=b&sig=qLKro%2BHDQrsO5f3FYZx%2Fwy9qlS%2BHd96F%2Bx%2B29p4Q1IQ%3D&st=2020-11-08T22%3A28%3A59Z&se=2020-11-09T06%3A38%3A59Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/reinforcementlearning.txt\"]], \"run_duration\": \"0:52:04\", \"cluster_coordination_timeout_seconds\": \"3600\"}, \"child_runs\": [{\"run_id\": \"deeprm_single_jobset_rllib_pg_1604867900_74b66959_head\", \"run_number\": 2, \"metric\": null, \"status\": \"Completed\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2020-11-08T20:38:47.813524Z\", \"end_time\": \"2020-11-08T21:30:02.256717Z\", \"created_time\": \"2020-11-08T20:38:32.835607Z\", \"created_time_dt\": \"2020-11-08T20:38:32.835607Z\", \"duration\": \"0:51:29\"}, {\"run_id\": \"deeprm_single_jobset_rllib_pg_1604867900_74b66959_worker\", \"run_number\": 3, \"metric\": null, \"status\": \"Completed\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2020-11-08T20:38:46.461889Z\", \"end_time\": \"2020-11-08T21:31:05.334338Z\", \"created_time\": \"2020-11-08T20:38:35.591594Z\", \"created_time_dt\": \"2020-11-08T20:38:35.591594Z\", \"duration\": \"0:52:29\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2020-11-08T20:38:25.5245738Z][Info]Starting reinforcement learning run with id deeprm_single_jobset_rllib_pg_1604867900_74b66959.\\n[2020-11-08T20:38:31.2414985Z][Info]Starting head node child run with id deeprm_single_jobset_rllib_pg_1604867900_74b66959_head.\\n[2020-11-08T20:38:33.2156370Z][Info]Starting worker child run with id deeprm_single_jobset_rllib_pg_1604867900_74b66959_worker.\\n[2020-11-08T21:30:32.8515010Z][Info]Some child runs have reached terminal state. All active child runs will be cancelled. The run Ids that reached terminal state are: deeprm_single_jobset_rllib_pg_1604867900_74b66959_head.\\n[2020-11-08T21:30:32.9153657Z][Info]Updating status of child run with Id deeprm_single_jobset_rllib_pg_1604867900_74b66959_worker from Running to Completed, since one of the child runs has reached a terminal state.\\n\\nRun is completed.\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.17.0\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "352c7c257db941a79488916ae6e019f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"loading\": true}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the run to complete before proceeding. The run may take about 1 hour to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'runId': 'deeprm_single_jobset_rllib_pg_1604867900_74b66959',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2020-11-08T20:38:48.97646Z',\n",
       " 'endTimeUtc': '2020-11-08T21:30:29.459505Z',\n",
       " 'properties': {},\n",
       " 'inputDatasets': [],\n",
       " 'outputDatasets': [],\n",
       " 'logFiles': {'azureml-logs/reinforcementlearning.txt': 'https://alvilcekmlwork8166615291.blob.core.windows.net/azureml/ExperimentRun/dcid.deeprm_single_jobset_rllib_pg_1604867900_74b66959/azureml-logs/reinforcementlearning.txt?sv=2019-02-02&sr=b&sig=gq1Itn4UBVsBsVFP4l8YL8b9kEAnvOpb%2FsHdcYIo7iU%3D&st=2020-11-08T20%3A28%3A33Z&se=2020-11-09T04%3A38%3A33Z&sp=r'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our *ReinforcementLearningEstimator* run has 2 child runs: one for the Ray head node and another for the Ray worker nodes. Here we get a reference to the first child run, where the Ray head node runs the RLLib trainer that performs the agent training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of child runs: 2\n",
      "Child run info:\n",
      "Run(Experiment: deeprm_single_jobset_rllib_pg,\n",
      "Id: deeprm_single_jobset_rllib_pg_1604867900_74b66959_head,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Completed)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "child_run_0 = None\n",
    "timeout = 30\n",
    "while timeout > 0 and not child_run_0:\n",
    "    child_runs = list(run.get_children())\n",
    "    print('Number of child runs:', len(child_runs))\n",
    "    if len(child_runs) > 0:\n",
    "        child_run_0 = child_runs[0]\n",
    "        break\n",
    "    time.sleep(2)\n",
    "    timeout -= 2\n",
    "\n",
    "print('Child run info:')\n",
    "print(child_run_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also simply use run id to get a handle to an in-progress or a previously concluded run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Run\n",
    "\n",
    "run_id = child_run_0.id\n",
    "child_run_0 = Run(exp, run_id=run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can list all logs and artifacts produced by the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['azureml-logs/55_azureml-execution-tvmps_77a89e2d10a78122301f1e03a5eb5955680510e70fa7a6dfbc980638e0a28980_d.txt',\n",
       " 'azureml-logs/55_azureml-execution-tvmps_c852d7abefb9b65c2f17d2264bc9d8a7088d2d5a6a65aa4c6f1bd792dd8907bc_d.txt',\n",
       " 'azureml-logs/55_azureml-execution-tvmps_ff97d8d8dc8a2150423c77c501e229a810d3513908bf597cf5cc6a3d83486869_d.txt',\n",
       " 'azureml-logs/65_job_prep-tvmps_77a89e2d10a78122301f1e03a5eb5955680510e70fa7a6dfbc980638e0a28980_d.txt',\n",
       " 'azureml-logs/65_job_prep-tvmps_c852d7abefb9b65c2f17d2264bc9d8a7088d2d5a6a65aa4c6f1bd792dd8907bc_d.txt',\n",
       " 'azureml-logs/65_job_prep-tvmps_ff97d8d8dc8a2150423c77c501e229a810d3513908bf597cf5cc6a3d83486869_d.txt',\n",
       " 'azureml-logs/70_driver_log.txt',\n",
       " 'azureml-logs/70_driver_log_0.txt',\n",
       " 'azureml-logs/70_driver_log_1.txt',\n",
       " 'azureml-logs/70_mpi_log.txt',\n",
       " 'azureml-logs/75_job_post-tvmps_77a89e2d10a78122301f1e03a5eb5955680510e70fa7a6dfbc980638e0a28980_d.txt',\n",
       " 'azureml-logs/process_info.json',\n",
       " 'azureml-logs/process_status.json',\n",
       " 'logs/azureml/0_177_azureml.log',\n",
       " 'logs/azureml/130_azureml.log',\n",
       " 'logs/azureml/1_129_azureml.log',\n",
       " 'logs/azureml/job_prep_azureml.log',\n",
       " 'logs/azureml/job_release_azureml.log',\n",
       " 'outputs/checkpoint_300/.is_checkpoint',\n",
       " 'outputs/checkpoint_300/checkpoint-300',\n",
       " 'outputs/checkpoint_300/checkpoint-300.tune_metadata',\n",
       " 'ray/dashboard_url',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/debug_state.txt',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/dashboard.err',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/dashboard.out',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/gcs_server.err',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/gcs_server.out',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/log_monitor.err',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/log_monitor.out',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/monitor.err',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/monitor.out',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/plasma_store.err',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/plasma_store.out',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/python-core-driver-0300ffffffffffffffffffffffffffffffffffff20201108-203938.285',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/python-core-worker-2c73aa836ca8a0b426110886214dabae318548ce20201108-203905.189',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/python-core-worker-ca65fbb908c00c77c148c9c915c76ac0726f75fc20201108-203905.186',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/python-core-worker-d829efd04b15cabcebf7f00ced3bfdf93fc2cd7220201108-203905.187',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/python-core-worker-df088ec38f2b5fbc56f865aaa309de20cc5cf02020201108-203905.188',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/python-core-worker-eca418f3f07310ae927bedc71291285a004f187520201108-203905.191',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/python-core-worker-f5d0043f116a88f60f108be00a4a2d43ace682be20201108-203905.190',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/raylet.err',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/raylet.out',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/redis-shard_0.err',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/redis-shard_0.out',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/redis-shard_1.err',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/redis-shard_1.out',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/redis.err',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/redis.out',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/reporter.err',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/reporter.out',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/worker-2459e8651746d47d2f440ba12fbcf3d5576e159c.err',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/worker-2459e8651746d47d2f440ba12fbcf3d5576e159c.out',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/worker-8cf78c7e3133ce5738c4602a1100b32f1ab88933.err',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/worker-8cf78c7e3133ce5738c4602a1100b32f1ab88933.out',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/worker-c03072fce69cd7d7a861f0e5fca60c5cbdcc35a1.err',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/worker-c03072fce69cd7d7a861f0e5fca60c5cbdcc35a1.out',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/worker-d08040055fe3efcd487b3d13c75c1ec5de329c10.err',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/worker-d08040055fe3efcd487b3d13c75c1ec5de329c10.out',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/worker-d3cb69ae86b4786162591364dd2f39a202aaaf39.err',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/worker-d3cb69ae86b4786162591364dd2f39a202aaaf39.out',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/worker-dd7f5ff9094c68169c0dfda0db5232446fae8a6a.err',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/logs/worker-dd7f5ff9094c68169c0dfda0db5232446fae8a6a.out',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/sockets/plasma_store',\n",
       " 'ray/session_2020-11-08_20-39-03_865856_145/sockets/raylet']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child_run_0.get_file_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training script, our model checkpoint is saved using the RLLib trainer. By default, the naming convention for the checkpoint file is *checkpoint-\\<iteration\\>*. Here we download the checkpoint files needed to recreate the trained agent later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_folder = '../model_checkpoints/single_jobset'\n",
    "os.makedirs(checkpoint_folder, exist_ok = True)\n",
    "\n",
    "for f in child_run_0.get_file_names():\n",
    "    f_name = os.path.basename(f)\n",
    "    if f_name.startswith('checkpoint-'):\n",
    "        child_run_0.download_file(f, checkpoint_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can find code snippets to clean up any resources created as part of this tutorial that you don't wish to retain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To archive the created experiment:\n",
    "#exp.archive()\n",
    "\n",
    "# To delete the compute targets:\n",
    "#head_compute_target.delete()\n",
    "#worker_compute_target.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
